{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMRM5GQ6MEb9FrQH7DnN0VH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bfestus/medical_chatbot_qa/blob/main/medical_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 1: Setup & Data Preparation"
      ],
      "metadata": {
        "id": "yOkWxp-FRdXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Medical_chatbot/medical_qa_doctor_style_refined.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Clean dataset\n",
        "df.drop_duplicates(inplace=True)\n",
        "df['question'] = df['question'].str.strip()\n",
        "df['answer'] = df['answer'].str.strip()\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Preview\n",
        "print(f\"Total cleaned samples: {len(df)}\")\n",
        "df = df.rename(columns={'question': 'input_text', 'answer': 'target_text'})\n",
        "df['input_text'] = 'healthcare question: ' + df['input_text']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6CvfWOURWL_",
        "outputId": "492beac1-259e-4c89-d40a-dc4c59ba41cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Total cleaned samples: 285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 2: Train/Val/Test Split"
      ],
      "metadata": {
        "id": "krIhKBqeRlT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80-10-10 split\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train_df),\n",
        "    'validation': Dataset.from_pandas(val_df),\n",
        "    'test': Dataset.from_pandas(test_df)\n",
        "})\n"
      ],
      "metadata": {
        "id": "ViVfLZPNRpGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 3: Tokenization"
      ],
      "metadata": {
        "id": "Zl4n0J6IRs7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess(example):\n",
        "    inputs = tokenizer(example['input_text'], max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
        "    targets = tokenizer(example['target_text'], max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess, batched=True, remove_columns=dataset['train'].column_names)\n"
      ],
      "metadata": {
        "id": "vKvlVNc2Rv2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 4: Model Setup"
      ],
      "metadata": {
        "id": "Pxnc6CicR3RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, create_optimizer\n",
        "\n",
        "# Load model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 8\n",
        "epochs = 30\n",
        "learning_rate = 5e-5\n",
        "num_train_steps = (len(tokenized_datasets['train']) // batch_size) * epochs\n",
        "optimizer, schedule = create_optimizer(init_lr=learning_rate, num_warmup_steps=0, num_train_steps=num_train_steps)\n"
      ],
      "metadata": {
        "id": "QxHthleLR4U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 5: TF Dataset Creation"
      ],
      "metadata": {
        "id": "Dv7TNp1USAQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset_from_hf(dataset, data_collator, batch_size, shuffle=False):\n",
        "    examples = [{\n",
        "        \"input_ids\": example[\"input_ids\"],\n",
        "        \"attention_mask\": example[\"attention_mask\"],\n",
        "        \"labels\": example[\"labels\"]\n",
        "    } for example in dataset]\n",
        "\n",
        "    def data_generator():\n",
        "        indices = list(range(len(examples)))\n",
        "        if shuffle:\n",
        "            np.random.shuffle(indices)\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            batch = [examples[j] for j in indices[i:i + batch_size]]\n",
        "            collated = data_collator(batch)\n",
        "            yield (\n",
        "                {\"input_ids\": np.array(collated[\"input_ids\"]), \"attention_mask\": np.array(collated[\"attention_mask\"])},\n",
        "                np.array(collated[\"labels\"])\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        data_generator,\n",
        "        output_signature=(\n",
        "            {\n",
        "                \"input_ids\": tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "                \"attention_mask\": tf.TensorSpec(shape=(None, None), dtype=tf.int32)\n",
        "            },\n",
        "            tf.TensorSpec(shape=(None, None), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Build TF datasets\n",
        "tf_train_dataset = create_tf_dataset_from_hf(tokenized_datasets[\"train\"], data_collator, batch_size, shuffle=True)\n",
        "tf_val_dataset = create_tf_dataset_from_hf(tokenized_datasets[\"validation\"], data_collator, batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "peKhVObGSHdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 6: Model Training & Saving"
      ],
      "metadata": {
        "id": "ZThHSswBSMWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train\n",
        "model.compile(optimizer=optimizer)\n",
        "model.fit(tf_train_dataset, validation_data=tf_val_dataset, epochs=epochs)\n",
        "\n",
        "# Save model\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/Medical_chatbot/healthcare-chatbot-model\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68g73xzDSauq",
        "outputId": "496b95f2-8c31-4480-8eec-1bb3fe08b246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "29/29 [==============================] - 89s 548ms/step - loss: 5.8964 - val_loss: 2.2403\n",
            "Epoch 2/30\n",
            "29/29 [==============================] - 4s 152ms/step - loss: 2.1928 - val_loss: 1.4714\n",
            "Epoch 3/30\n",
            "29/29 [==============================] - 5s 161ms/step - loss: 1.5327 - val_loss: 0.9013\n",
            "Epoch 4/30\n",
            "29/29 [==============================] - 4s 136ms/step - loss: 0.9885 - val_loss: 0.4240\n",
            "Epoch 5/30\n",
            "29/29 [==============================] - 4s 128ms/step - loss: 0.6011 - val_loss: 0.2382\n",
            "Epoch 6/30\n",
            "29/29 [==============================] - 4s 140ms/step - loss: 0.3976 - val_loss: 0.1600\n",
            "Epoch 7/30\n",
            "29/29 [==============================] - 4s 126ms/step - loss: 0.3083 - val_loss: 0.1145\n",
            "Epoch 8/30\n",
            "29/29 [==============================] - 3s 117ms/step - loss: 0.2499 - val_loss: 0.0908\n",
            "Epoch 9/30\n",
            "29/29 [==============================] - 3s 119ms/step - loss: 0.2132 - val_loss: 0.0756\n",
            "Epoch 10/30\n",
            "29/29 [==============================] - 3s 120ms/step - loss: 0.1986 - val_loss: 0.0658\n",
            "Epoch 11/30\n",
            "29/29 [==============================] - 3s 117ms/step - loss: 0.1707 - val_loss: 0.0592\n",
            "Epoch 12/30\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 0.1614 - val_loss: 0.0554\n",
            "Epoch 13/30\n",
            "29/29 [==============================] - 4s 122ms/step - loss: 0.1442 - val_loss: 0.0526\n",
            "Epoch 14/30\n",
            "29/29 [==============================] - 3s 113ms/step - loss: 0.1350 - val_loss: 0.0506\n",
            "Epoch 15/30\n",
            "29/29 [==============================] - 4s 122ms/step - loss: 0.1274 - val_loss: 0.0495\n",
            "Epoch 16/30\n",
            "29/29 [==============================] - 3s 114ms/step - loss: 0.1186 - val_loss: 0.0481\n",
            "Epoch 17/30\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 0.1167 - val_loss: 0.0471\n",
            "Epoch 18/30\n",
            "29/29 [==============================] - 3s 115ms/step - loss: 0.1107 - val_loss: 0.0456\n",
            "Epoch 19/30\n",
            "29/29 [==============================] - 3s 117ms/step - loss: 0.1080 - val_loss: 0.0450\n",
            "Epoch 20/30\n",
            "29/29 [==============================] - 3s 113ms/step - loss: 0.1066 - val_loss: 0.0453\n",
            "Epoch 21/30\n",
            "29/29 [==============================] - 3s 120ms/step - loss: 0.1023 - val_loss: 0.0454\n",
            "Epoch 22/30\n",
            "29/29 [==============================] - 3s 115ms/step - loss: 0.1020 - val_loss: 0.0446\n",
            "Epoch 23/30\n",
            "29/29 [==============================] - 3s 115ms/step - loss: 0.1010 - val_loss: 0.0446\n",
            "Epoch 24/30\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 0.0998 - val_loss: 0.0445\n",
            "Epoch 25/30\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 0.0996 - val_loss: 0.0440\n",
            "Epoch 26/30\n",
            "29/29 [==============================] - 3s 109ms/step - loss: 0.0959 - val_loss: 0.0437\n",
            "Epoch 27/30\n",
            "29/29 [==============================] - 3s 113ms/step - loss: 0.0978 - val_loss: 0.0435\n",
            "Epoch 28/30\n",
            "29/29 [==============================] - 3s 115ms/step - loss: 0.0992 - val_loss: 0.0435\n",
            "Epoch 29/30\n",
            "29/29 [==============================] - 3s 110ms/step - loss: 0.0976 - val_loss: 0.0435\n",
            "Epoch 30/30\n",
            "29/29 [==============================] - 3s 112ms/step - loss: 0.0953 - val_loss: 0.0435\n",
            "Model saved to /content/drive/MyDrive/Colab Notebooks/Medical_chatbot/healthcare-chatbot-model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 7: Inference Function"
      ],
      "metadata": {
        "id": "_3Ov60RxTyPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keywords for filtering\n",
        "medical_keywords = [\n",
        "    \"symptom\", \"diagnose\", \"treatment\", \"medicine\", \"disease\", \"doctor\",\n",
        "    \"covid\", \"cancer\", \"diabetes\", \"bipolar\", \"stroke\", \"fever\", \"infection\",\n",
        "    \"pain\", \"mental\", \"health\", \"hospital\", \"vaccine\", \"prescription\",\n",
        "    \"disorder\", \"diagnosed\", \"asthma\", \"epilepsy\", \"hypertension\",\n",
        "    \"depression\", \"anxiety\", \"hiv\", \"ibuprofen\", \"lisinopril\", \"side effects\",\n",
        "    \"paracetamol\", \"atorvastatin\", \"metformin\", \"checkup\", \"healthy lifestyle\",\n",
        "    \"symptoms\", \"water\", \"dose\", \"blood pressure\", \"heart\", \"immune\",\n",
        "    \"medication\", \"mental health\", \"therapy\"\n",
        "]\n",
        "\n",
        "def is_medical_question(question):\n",
        "    return any(keyword in question.lower() for keyword in medical_keywords)\n",
        "\n",
        "def generate_answer(question):\n",
        "    if not is_medical_question(question):\n",
        "        return \"❗ Sorry, I can only answer healthcare-related questions.\"\n",
        "    input_text = \"healthcare question: \" + question\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"tf\", padding=True, truncation=True).input_ids\n",
        "    output = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Test examples\n",
        "test_questions = [\n",
        "    \"What are the symptoms of stroke?\",\n",
        "    \"Can bipolar disorder be detected early?\",\n",
        "    \"How is COVID-19 diagnosed?\",\n",
        "]\n",
        "\n",
        "for q in test_questions:\n",
        "    print(f\"\\n❓ Question: {q}\")\n",
        "    print(f\"💬 Answer: {generate_answer(q)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcYHY_UfT06a",
        "outputId": "a101efca-ea39-4ab4-a42e-b944efe22d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❓ Question: What are the symptoms of stroke?\n",
            "💬 Answer: Symptoms of stroke include shortness of breath, fatigue. this information is helpful for understanding the condition better. understanding this response helps in gaining deeper insight into the medical condition and encourages timely medical consultation. If you have any concerns or symptoms, it's important to follow up with a healthcare provider for a personalized evaluation. If you have any concerns or symptoms, it's important to follow up with a healthcare provider for a personalized evaluation.\n",
            "\n",
            "❓ Question: Can bipolar disorder be detected early?\n",
            "💬 Answer: Yes, certainly, early detection of bipolar disorder is possible through questionnaires, ecg. this information is helpful for understanding the condition better. healthcare providers use behavioral assessments, interviews, and family history to evaluate and detect mood disorders at an early stage, potentially preventing more severe episodes. If you have any concerns or symptoms, it's important to follow up with a healthcare provider for a personalized evaluation. If you have any concerns or symptoms, it's important to follow up with a healthcare provider for a personalized evaluation.\n",
            "\n",
            "❓ Question: How is COVID-19 diagnosed?\n",
            "💬 Answer: Covid-19 is diagnosed using mri scans, ecg. this information is helpful for understanding the condition better. understanding this response helps in gaining deeper insight into the medical condition and encourages timely medical consultation. If you have any concerns or symptoms, it's important to follow up with a healthcare provider for a personalized evaluation. If you have any concerns or symptoms, it's important to follow up with a healthcare provider for a personalized evaluation.\n"
          ]
        }
      ]
    }
  ]
}